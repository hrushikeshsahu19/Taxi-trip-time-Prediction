# -*- coding: utf-8 -*-
"""Hrushikesh Sahu - Team 3 - Capstone Project 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16x4HV6tSkzXL7Vm5-msv4j275SxOn-7h

# <b><u> Project Title : Taxi trip time Prediction : Predicting total ride duration of taxi trips in New York City</u></b>

## <b> Problem Description </b>

### Your task is to build a model that predicts the total ride duration of taxi trips in New York City. Your primary dataset is one released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coordinates, number of passengers, and several other variables.

## <b> Data Description </b>

### The dataset is based on the 2016 NYC Yellow Cab trip record data made available in Big Query on Google Cloud Platform. The data was originally published by the NYC Taxi and Limousine Commission (TLC). The data was sampled and cleaned for the purposes of this project. Based on individual trip attributes, you should predict the duration of each trip in the test set.

### <b>NYC Taxi Data.csv</b> - the training set (contains 1458644 trip records)


### Data fields
* #### id - a unique identifier for each trip
* #### vendor_id - a code indicating the provider associated with the trip record
* #### pickup_datetime - date and time when the meter was engaged
* #### dropoff_datetime - date and time when the meter was disengaged
* #### passenger_count - the number of passengers in the vehicle (driver entered value)
* #### pickup_longitude - the longitude where the meter was engaged
* #### pickup_latitude - the latitude where the meter was engaged
* #### dropoff_longitude - the longitude where the meter was disengaged
* #### dropoff_latitude - the latitude where the meter was disengaged
* #### store_and_fwd_flag - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip
* #### trip_duration - duration of the trip in seconds
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing the libraries
import numpy as np
import pandas as pd
from numpy import math
import xgboost
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings("ignore")
import warnings
from pylab import rcParams
# %matplotlib inline
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

# Importing the dataset
dataset = pd.read_csv('/content/drive/MyDrive/AlmaBetter/Cohort Aravali/Module 4/Week 1/NYC Taxi Data.csv')

dataset.head()

dataset.shape

"""###**it has 1458644 rows and 11 columns**"""

dataset.info()

"""###***Threre is No null values in our dataset***"""

# duplicate
len(dataset[dataset.duplicated()])

"""###***Threre is No duplicate values in our dataset***"""

dataset.describe()

dataset[dataset['trip_duration']==3.526282e+06]

"""###***There are no numerical columns with missing data***
###***The passenger count varies between 1 and 9 with most people number of people being 1 or 2***
###***The trip duration varying from 1s to 1939736s or 538 hrs. There are definitely some outliers present which must be treated.***
"""

list(dataset.columns)

"""###***Name of the columns in our dataset is***
 'id',
 'vendor_id',
 'pickup_datetime',
 'dropoff_datetime',
 'passenger_count',
 'pickup_longitude',
 'pickup_latitude',
 'dropoff_longitude',
 'dropoff_latitude',
 'store_and_fwd_flag',
 'trip_duration'
"""

dataset['vendor_id'].value_counts().reset_index()

dataset['passenger_count'].value_counts().reset_index()

dataset['store_and_fwd_flag'].value_counts().reset_index()

"""#**Exploratory Data Analysis**"""

# Dependent variable 'trip_duration'
plt.figure(figsize=(15,10))
plt.title('Distribution of trip_duration')
sns.distplot(dataset['trip_duration'],color="B")

plt.figure(figsize=(15,10))
plt.title('Log of trip_duration')
sns.distplot(np.log10(dataset['trip_duration']),color="B")

"""##***take the log of trip_duration and plot the distribution   its looks like symmetric distributions***"""

#plot to see the differences - minimal, but there is some change
n = 100000 
f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10, 5))
ax1.scatter(dataset.pickup_longitude[:n], 
            dataset.pickup_latitude[:n],
            alpha = 0.1)
ax1.set_title('Pickup')
ax2.scatter(dataset.dropoff_longitude[:n], 
            dataset.dropoff_latitude[:n],
            alpha = 0.1)
ax2.set_title('Dropoff')

def select_within_boundingbox(df, BB):
    return ((df["pickup_longitude"] >= BB[0]) & (df["pickup_longitude"] <= BB[1]) & 
            (df["pickup_latitude"] >= BB[2]) & (df["pickup_latitude"] <= BB[3]) &
            (df["dropoff_longitude"] >= BB[0]) & (df["dropoff_longitude"] <= BB[1]) &
            (df["dropoff_latitude"] >= BB[2]) & (df["dropoff_latitude"] <= BB[3]))

BB = (-74.3, -73.0, 40.6, 41.7)

nyc_map = plt.imread('/content/drive/MyDrive/AlmaBetter/Cohort Aravali/Module 4/Week 1/nyc.webp')

dataset = dataset[select_within_boundingbox(dataset, BB)]

def plot_on_map(df, BB, nyc_map, s=10, alpha=0.2):
    fig, axs = plt.subplots(1,2,figsize=(13, 8))
    axs[0].scatter(df["pickup_longitude"], df["pickup_latitude"], alpha = alpha, c='g', s=s)
    axs[0].set_xlim((BB[0], BB[1]))
    axs[0].set_ylim((BB[2], BB[3]))
    axs[0].set_title('Pickup Locations')
    axs[0].imshow(nyc_map, extent=BB)
    
    axs[1].scatter(df["dropoff_longitude"], df["dropoff_latitude"] , alpha = alpha, c='r', s=s)
    axs[1].set_xlim((BB[0], BB[1]))
    axs[1].set_ylim((BB[2], BB[3]))
    axs[1].set_title('Dropoff Locations')
    axs[1].imshow(nyc_map, extent=BB)

plot_on_map(dataset, BB, nyc_map, s=3, alpha=1)

"""###***from above plot we can conclude that there are some outlier present in our garph because some of the data point are present inside the river***"""

nyc_map = plt.imread('/content/drive/MyDrive/AlmaBetter/Cohort Aravali/Module 4/Week 1/NYC_MOST.jpg')

plot_on_map(dataset, BB, nyc_map, s=3, alpha=1)

passenger_count_df=dataset['passenger_count'].value_counts().reset_index()

passenger_count_df.rename(columns={'index': 'Number_of_passenger'}, inplace=True)
passenger_count_df.head()

plt.figure(figsize=(13,8))
plt.title('passenger types')
sns.barplot(x='Number_of_passenger',y='passenger_count',data=passenger_count_df)

"""###**from above  graph we can conclude that most of the taxi have single passenger**"""

vendor_id_df=dataset['vendor_id'].value_counts().reset_index()
vendor_id_df.rename(columns={'index': 'Vendor_type'}, inplace=True)
vendor_id_df.rename(columns={'vendor_id': 'total_number_of_vendor'}, inplace=True)
vendor_id_df

plt.figure(figsize=(13,8))
plt.title('Types of Vendor')
sns.barplot(x='Vendor_type',y='total_number_of_vendor',data=vendor_id_df)

store_and_fwd_flag_df=dataset['store_and_fwd_flag'].value_counts().reset_index()
store_and_fwd_flag_df.rename(columns={'index': 'store_and_fwd_flag_Type'}, inplace=True)
store_and_fwd_flag_df.rename(columns={'store_and_fwd_flag': 'store_and_fwd_flag_Count'}, inplace=True)
store_and_fwd_flag_df

plt.figure(figsize=(13,8))
plt.title('store and fwd flag Type')
sns.barplot(x='store_and_fwd_flag_Type',y='store_and_fwd_flag_Count',data=store_and_fwd_flag_df)

"""###**from this graph we can observed that store_and_fwd_flag  value 'N' have  most of the observation**"""

# InDependent variable 'pickup_longitude'
plt.figure(figsize=(15,10))
plt.title('distribution of pickup_longitude')
sns.distplot(dataset['pickup_longitude'],color="B")

# InDependent variable 'pickup_latitude'
plt.figure(figsize=(15,10))
plt.title('distribution of pickup_latitude')
sns.distplot(dataset['pickup_latitude'],color="B")

# InDependent variable 'dropoff_longitude'
plt.figure(figsize=(15,10))
plt.title('Distribution of dropoff_longitude')
sns.distplot(dataset['dropoff_longitude'],color="B")

# InDependent variable 'dropoff_latitude'
plt.figure(figsize=(15,10))
plt.title('distribution of dropoff_latitude')
sns.distplot(dataset['dropoff_latitude'],color="B")

numeric_features = dataset.describe().columns
numeric_features

# plot a bar plot for each categorical feature count (except car_ID)

for col in numeric_features:
    fig = plt.figure(figsize=(9, 6))
    ax = fig.gca()
    feature = dataset[col]
    feature.hist(bins=50, ax = ax)
    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)
    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)    
    ax.set_title(col)
plt.show()

"""#****Transformation****
###***square-root for moderate skew: sqrt(x) for positively skewed data, sqrt(max(x+1) - x) for negatively skewed data***

###***log for greater skew: log10(x) for positively skewed data, log10(max(x+1) - x) for negatively skewed data***

###***inverse for severe skew: 1/x for positively skewed data 1/(max(x+1) - x) for negatively skewed data***

###***Linearity and heteroscedasticity: First try log transformation in a situation where the dependent variable starts to increase more rapidly with increasing independent variable values If your data does the opposite – dependent variable values decrease more rapidly with increasing independent variable values – you can first consider a square transformation.***
"""

for col in numeric_features:
    fig = plt.figure(figsize=(9, 6))
    ax = fig.gca()
    feature = dataset[col]
    label = dataset['trip_duration']
    correlation = feature.corr(label)
    plt.scatter(x=feature, y=label)
    plt.xlabel(col)
    plt.ylabel('trip_duration')
    ax.set_title('trip_duration vs ' + col + '- correlation: ' + str(correlation))
    z = np.polyfit(dataset[col], dataset['trip_duration'], 1)
    y_hat = np.poly1d(z)(dataset[col])

    plt.plot(dataset[col], y_hat, "r--", lw=1)

plt.show()

## Correlation
plt.figure(figsize=(15,10))
correlation = dataset.corr()
sns.heatmap(abs(correlation), annot=True, cmap='coolwarm')

"""
##***Correlation is a term that is a measure of the strength of a linear relationship between two quantitative variables (e.g., height, weight)***

##***it is 3 types***

##***1.positive correlation :A positive correlation exists when one variable decreases as the other variable decreases, or one variable increases while the other increases***

##***2.Negative correlation : Negative correlation is a relationship between two variables in which one variable increases as the other decreases, and vice versa. In statistics, a perfect negative correlation is represented by the value -1***

##***3.Zero correlation :0 indicates no correlation***

##***In statistics, a perfect positive correlation is represented by the correlation coefficient value +1.0, while 0 indicates no correlation, and -1.0 indicates a perfect inverse (negative) correlation***
"""

#Multicollinearity
from statsmodels.stats.outliers_influence import variance_inflation_factor
def calc_vif(X):
 
   # Calculating VIF
   vif = pd.DataFrame()
   vif["variables"] = X.columns
   vif["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
 
   return(vif)

calc_vif(dataset[[i for i in dataset.describe().columns if i not in ['trip_duration','pickup_longitude','dropoff_longitude']]])

"""###indepandent variable are highly correlated to each other """

categorical_features = dataset.describe(include=['object','category']).columns

dataset.head()

from math import sin, cos, sqrt, atan2, radians

def get_distance(lon_1, lon_2, lat_1, lat_2):

    # approximate radius of earth in km
    R = 6373.0

    lat1 = radians(lat_1)
    lon1 = radians(lon_1)
    lat2 = radians(lat_2)
    lon2 = radians(lon_2)

    dlon = lon2 - lon1
    dlat = lat2 - lat1

    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))

    distance = R * c

    return distance

"""##***above function  help us to find the distance between (pickup_longitude,pickup_latitude) and (dropoff_longitude,dropoff_latitude)***"""

dataset["distance"] = dataset.apply(lambda x: get_distance(x["pickup_longitude"],x["dropoff_longitude"],x["pickup_latitude"],x["dropoff_latitude"]),axis=1)

"""###***created a new features distance from existing features***"""

# insight : it seems most of the cab rides took with in 10kms by the cab passengers . 
bin_0 =  dataset.loc[(dataset['distance'] == 0), ['distance']]
bin_1 =  dataset.loc[(dataset['distance']>0)&(dataset['distance']<=10),['distance']]
bin_2 =  dataset.loc[(dataset['distance']>10)&(dataset['distance']<=50),['distance']]
bin_3 =  dataset.loc[(dataset['distance']>50)&(dataset['distance']<=100),['distance']]
bin_4 = dataset.loc[(dataset['distance']>100)&(dataset['distance']<=250),['distance']]
bin_5 = dataset.loc[(dataset['distance']>250),['distance']]
bin_0['bins'] = '0'
bin_1['bins'] = '1-10'
bin_2['bins'] = '11-50'
bin_3['bins'] = '51-100'
bin_4['bins'] = '101-250'
bin_5['bins'] = '>250'
dist_bins = pd.concat([bin_0,bin_1,bin_2,bin_3,bin_4,bin_5],axis = 0)
plt.figure(figsize=(9,6))
sns.countplot(dist_bins['bins'])
plt.xlabel("Distance bins",fontsize = 15)
plt.show()

"""###**from this graph we can conclude that most of the Person  booking  Texi for distance 1 to 10**"""

dataset.head()

dataset.describe()

# InDependent variable 'distance'
plt.figure(figsize=(15,10))
plt.title('Distribution of distnace')
sns.distplot(dataset['distance'],color="B")

"""##***plot the distribution of 'distance' and it looks like postively skew***

###***This graph looks likes positive skew distribution because most of values are lies in left part of our graph***

###***positively skewed (or right-skewed) distribution is a type of distribution in which most values are clustered around the left tail of the distribution while the right tail of the distribution is longer.hear mean>median>mode***

###***negatively skewed (also known as left-skewed) distribution is a type of distribution in which more values are concentrated on the right side (tail) of the distribution graph while the left tail of the distribution graph is longer.hear mean<median<mode***
"""

import folium
f = folium.Figure(width=1500, height=500)
mapa = folium.Map(location = (40.7679, -73.9822), zoom_start=11).add_to(f)

for index, row in dataset.sample(1000).iterrows():
    folium.Marker([row["pickup_latitude"], row["pickup_longitude"]], icon=folium.Icon(color="blue")).add_to(mapa)
    folium.Marker([row["dropoff_latitude"], row["dropoff_longitude"]], icon=folium.Icon(color="red")).add_to(mapa)


display(mapa)

import folium
f = folium.Figure(width=1500, height=500)
mapa = folium.Map(location = (40.7679, -73.9822), zoom_start=11).add_to(f)

for index, row in dataset[dataset["distance"]>20].sample(200).iterrows():
    folium.Marker([row["pickup_latitude"], row["pickup_longitude"]], icon=folium.Icon(color="blue")).add_to(mapa)
    folium.Marker([row["dropoff_latitude"], row["dropoff_longitude"]], icon=folium.Icon(color="red")).add_to(mapa)


display(mapa)

plt.figure(figsize=(15,8))
plt.title("Box plot of  distance ")
ax = sns.boxplot(data=dataset['distance'], orient="v")

"""###**plot the box plot of features distance  for outlier**

#**IQR=Q3-Q1**

#**lower_limit_outlier=Q1-1.5*IQR**

#**upper_limit_outlier=Q3+1.5*IQR**

##**so we have to take the values which is greater then lower limit outlier and less then upper limit outlier**

#**remove the outlier present in distance**
"""

percentile_q1 = np.percentile(dataset['distance'],25)
print(percentile_q1)
percentile_q2 = np.percentile(dataset['distance'],50)
print(percentile_q2)
percentile_q3 = np.percentile(dataset['distance'],75)
print(percentile_q3)

iqr=percentile_q3 - percentile_q1
lower_limit_outlier=percentile_q1-1.5*iqr
upper_limit_outlier=percentile_q3+1.5*iqr

print("lower limit for outlier  :",lower_limit_outlier)
print("Upper limit for outlier  :",upper_limit_outlier)

dataset=dataset[dataset['distance']>0]
dataset=dataset[dataset['distance']<upper_limit_outlier]

"""###**distance can not be negative,so i have take minimum distance is equal to 0**"""

dataset.shape

"""##***after removeing outlier from distance columns the shape of our dataset reduced to (1312074, 12)***"""

plt.figure(figsize=(15,8))
plt.title("Box plot of  trip_duration ")
ax = sns.boxplot(data=dataset['trip_duration'], orient="v")

percentile_q1_trip_duration = np.percentile(dataset['trip_duration'],25)
print(percentile_q1_trip_duration)
percentile_q2_trip_duration = np.percentile(dataset['trip_duration'],50)
print(percentile_q2_trip_duration)
percentile_q3_trip_duration = np.percentile(dataset['trip_duration'],75)
print(percentile_q3_trip_duration)

iqr=percentile_q3_trip_duration - percentile_q1_trip_duration
lower_limit_outlier_trip_duration=percentile_q1_trip_duration-1.5*iqr
upper_limit_outlier_trip_duration=percentile_q3_trip_duration+1.5*iqr

print("lower limit for outlier  :",lower_limit_outlier_trip_duration)
print("Upper limit for outlier  :",upper_limit_outlier_trip_duration)

dataset=dataset[dataset['trip_duration']>0]
dataset=dataset[dataset['trip_duration']<upper_limit_outlier_trip_duration]

dataset.shape

"""##***after removeing outlier from distance columns the shape of our dataset reduced to (1271808, 12)***"""

dataset.head()

plt.figure(figsize=(15,8))
plt.title("Box plot of  passenger_count ")
ax = sns.boxplot(data=dataset['passenger_count'], orient="v")

percentile_q1_passenger_count = np.percentile(dataset['passenger_count'],25)
print(percentile_q1_passenger_count)
percentile_q2_passenger_count = np.percentile(dataset['passenger_count'],50)
print(percentile_q2_passenger_count)
percentile_q3_passenger_count = np.percentile(dataset['passenger_count'],75)
print(percentile_q3_passenger_count)

iqr=percentile_q3_passenger_count - percentile_q1_passenger_count
lower_limit_outlier_passenger_count=percentile_q1_passenger_count-1.5*iqr
upper_limit_outlier_passenger_count=percentile_q3_passenger_count+1.5*iqr

print("lower limit for outlier  :",lower_limit_outlier_passenger_count)
print("Upper limit for outlier  :",upper_limit_outlier_passenger_count)

dataset=dataset[dataset['passenger_count']>0]
dataset=dataset[dataset['passenger_count']<upper_limit_outlier_passenger_count]

dataset.shape

"""##***after removeing outlier from distance columns the shape of our dataset reduced to (1029043, 12)***"""

dataset.head()

dataset["pickup_datetime"] = pd.to_datetime(dataset["pickup_datetime"], format="%Y-%m-%d %H:%M:%S")

dataset['Day']=dataset['pickup_datetime'].dt.day_name()

dataset["year"] = dataset["pickup_datetime"].apply(lambda x: x.year)
dataset["month"] = dataset["pickup_datetime"].apply(lambda x: x.month)
dataset["day_num"] = dataset["pickup_datetime"].apply(lambda x: x.day)
dataset["hour"] = dataset["pickup_datetime"].apply(lambda x: x.hour)
dataset["minute"] = dataset["pickup_datetime"].apply(lambda x: x.minute)

"""##**create some new features  year,month,day,hour,minitue from pickup_datetime**"""

dataset['trip_duration_hour']=dataset['trip_duration']/3600 
dataset['speed']=dataset['distance']/dataset['trip_duration_hour']

"""##***create some new features trip_duration_hour from trip_duration and  speed from distance and time***"""

dataset.head()

dataset['month'].unique()

"""##***unique month of our dataset are [3, 6, 1, 4, 5, 2]***"""

dataset['year'].unique()

"""##**unique year of our dataset is 2016**"""

dataset['day_num'].unique()

"""##***unique date of our dataset are [14, 12, 19,  6, 26, 30, 17, 21, 27, 10, 15,  1, 16, 11,  5, 28,  9, 25, 20, 13, 23,  3, 18,  7,  2, 22, 29, 24,  4, 31,  8]***"""

dataset['hour'].unique()

"""##***unique month of our dataset are [17,  0, 11, 19, 13, 22,  7, 23, 21,  9, 20, 15,  8,  2, 12,  3, 10,   14, 16,  1, 18,  6,  5,  4]***"""

dataset['minute'].unique()

day_df=dataset.groupby('Day')['id'].count().reset_index()
day_df.rename(columns={'id': 'Number of Booking'}, inplace=True)
day_df

plt.figure(figsize=(9,6))
plt.title(' Number of texi book per day')
sns.barplot(x='Day',y='Number of Booking',data=day_df)

"""##***from above graph we can see that in saturday people are booking Taxi more number of times and in Monday People are bookiung texi less numbers times***"""

month_df=dataset.groupby('month')['id'].count().reset_index()
month_df.rename(columns={'id': 'Number of Booking'}, inplace=True)
month_df



plt.figure(figsize=(9,6))
plt.title('Texi Booking per Month')
sns.barplot(x='month',y='Number of Booking',data=month_df)

"""##***from this graph we can conclude that more texi is booking in the month of 3(March) and least booking are in the month 1(january)***"""

day_df=dataset.groupby('day_num')['id'].count().reset_index()
day_df

plt.figure(figsize=(15,10))
plt.title('Texi book per day')
sns.barplot(x='day_num',y='id',data=day_df)

hour_df=dataset.groupby('hour')['id'].count().reset_index()
hour_df.rename(columns={'id': 'Number_of_Booking'}, inplace=True)
hour_df

hour_df.sort_values(by='Number_of_Booking', axis=0, ascending=False, inplace=True)

plt.figure(figsize=(9,6))
plt.title('Most of the Texi booking  hour')
sns.barplot(x='hour',y='Number_of_Booking',data=hour_df[:5])

"""###**from above graph we can conclude that most of the people booking for Texi in evening time that is 6PM to 7PM**"""

hour_df.sort_values(by='Number_of_Booking', axis=0, ascending=True, inplace=True)

hour_df

plt.figure(figsize=(9,6))
plt.title('Least Texi booking  hour')
sns.barplot(x='hour',y='Number_of_Booking',data=hour_df[:5])

"""##**Least Taxi booing hour is 2,3,4,5,6**"""



minute_df=dataset.groupby('minute')['id'].count().reset_index()
minute_df.rename(columns={'id': 'Number_of_Booking'}, inplace=True)
minute_df

minute_df.sort_values(by=['Number_of_Booking'], inplace=True,ascending=False,axis=0)

minute_df

plt.figure(figsize=(17,13))
plt.title('Texi booking per top 10 minute')
sns.barplot(x='minute',y='Number_of_Booking',data=minute_df[:10])

dataset.head()

# One hot encoding
dataset= pd.get_dummies(dataset, columns=["store_and_fwd_flag", "Day"], prefix=["store_and_fwd_flag",'Day'])

dataset.head()

## Correlation
plt.figure(figsize=(30,13))
correlation = dataset.corr()
sns.heatmap(abs(correlation), annot=True, cmap='coolwarm')

"""
##***Correlation is a term that is a measure of the strength of a linear relationship between two quantitative variables (e.g., height, weight)***

##***it is 3 types***

##***1.positive correlation :A positive correlation exists when one variable decreases as the other variable decreases, or one variable increases while the other increases***

##***2.Negative correlation : Negative correlation is a relationship between two variables in which one variable increases as the other decreases, and vice versa. In statistics, a perfect negative correlation is represented by the value -1***

##***3.Zero correlation :0 indicates no correlation***

##***In statistics, a perfect positive correlation is represented by the correlation coefficient value +1.0, while 0 indicates no correlation, and -1.0 indicates a perfect inverse (negative) correlation***
"""

dataset.to_csv('/content/drive/MyDrive/AlmaBetter/Cohort Aravali/Module 4/Week 1/NEW_NYK_Texi_data.csv')

# Importing the dataset
df = pd.read_csv('/content/drive/MyDrive/AlmaBetter/Cohort Aravali/Module 4/Week 1/NEW_NYK_Texi_data.csv')

df.head()

features=['vendor_id','passenger_count','pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude','distance','month','hour','minute',
          'store_and_fwd_flag_N','store_and_fwd_flag_Y','Day_Friday','Day_Monday','Day_Saturday','Day_Sunday','Day_Thursday','Day_Tuesday','Day_Wednesday']

from scipy.stats import zscore
#Train test split
# numeric_cols = carprice.select_dtypes(include=[np.number]).columns
X = df[features].apply(zscore)[:100000]
y=df['trip_duration_hour'][:100000]

X[:5]

# Importing train_test_split
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

"""#**Applying  DecisionTreeRegressor to our DataSet**"""

# Maximum depth of trees
max_depth = [4,6,8,10]

# Minimum number of samples required to split a node
min_samples_split = [10,20,30]

# Minimum number of samples required at each leaf node
min_samples_leaf = [10,16,20]

# HYperparameter Grid
param_dt = {
              'max_depth' : max_depth,
              'min_samples_split' : min_samples_split,
              'min_samples_leaf' : min_samples_leaf}

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV

dt_model = DecisionTreeRegressor()

# Grid search
dt_grid = GridSearchCV(estimator=dt_model,
                       param_grid = param_dt,
                       cv = 5, verbose=2, scoring='r2')

dt_grid.fit(X_train,y_train)

dt_grid.best_score_

dt_grid.best_estimator_

dt_optimal_model =dt_grid.best_estimator_

y_pred_dt_test=dt_optimal_model.predict(X_test)
y_pred_dt_train=dt_optimal_model.predict(X_train)

plt.figure(figsize=(8,5))
plt.title('Train Data')
plt.plot(y_pred_dt_train)
plt.plot(np.array(y_train))
plt.legend(["Predicted","Actual"])
plt.show()

MSE  = mean_squared_error(y_train, y_pred_dt_train)
print("Train MSE :" , MSE)

RMSE = np.sqrt(MSE)
print("Train RMSE :" ,RMSE)

r2 = r2_score(y_train, y_pred_dt_train)
print("Train R2 :" ,r2)
print("Train Adjusted R2 : ",1-(1-r2_score(y_train, y_pred_dt_train))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)))

plt.figure(figsize=(8,5))
plt.title('Test Data')
plt.plot(y_pred_dt_test)
plt.plot(np.array(y_test))
plt.legend(["Predicted","Actual"])
plt.show()

MSE  = mean_squared_error(y_test, y_pred_dt_test)
print("Test MSE :" , MSE)

RMSE = np.sqrt(MSE)
print("Test RMSE :" ,RMSE)

r2 = r2_score(y_test, y_pred_dt_test)
print("Test R2 :" ,r2)
print("Test Adjusted R2 : ",1-(1-r2_score(y_test, y_pred_dt_test))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))

X_train.columns

dt_optimal_model.feature_importances_

importances = dt_optimal_model.feature_importances_

importance_dict = {'Feature' : list(X_train.columns),
                   'Feature Importance' : importances}

importance_df = pd.DataFrame(importance_dict)

importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)
importance_df

importance_df.sort_values(by=['Feature Importance'],ascending=False,inplace=True)
importance_df

plt.figure(figsize=(15,13))
plt.title('Features importace')
sns.barplot(x='Feature',y="Feature Importance",data=importance_df[:10])



"""#**Applying Lasso Regression to our DataSet**"""

from sklearn.model_selection import GridSearchCV
### Cross validation
lasso = Lasso()
parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}
lasso_regressor = GridSearchCV(lasso, parameters, scoring='r2', cv=5)
lasso_regressor.fit(X_train, y_train)

lasso_regressor.score(X_train, y_train)

print("The best fit alpha value is found out to be :" ,lasso_regressor.best_params_)
print("\nUsing ",lasso_regressor.best_params_, " the negative mean squared error is: ", lasso_regressor.best_score_)

y_pred_lasso = lasso_regressor.predict(X_test)
y_pred_lasso_train = lasso_regressor.predict(X_train)

plt.figure(figsize=(8,5))
plt.title('Train Data')
plt.plot(y_pred_lasso_train)
plt.plot(np.array(y_train))
plt.legend(["Predicted","Actual"])
plt.show()

MSE  = mean_squared_error(y_train, y_pred_lasso_train)
print("Train MSE :" , MSE)

RMSE = np.sqrt(MSE)
print("Train RMSE :" ,RMSE)

r2 = r2_score(y_train, y_pred_lasso_train)
print("Train R2 :" ,r2)
print("Train Adjusted R2 : ",1-(1-r2_score(y_train, y_pred_lasso_train))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)))

plt.figure(figsize=(8,5))
plt.title('Test Data')
plt.plot(y_pred_lasso)
plt.plot(np.array(y_test))
plt.legend(["Predicted","Actual"])
plt.show()

MSE  = mean_squared_error(y_test, y_pred_lasso)
print("Test MSE :" , MSE)

RMSE = np.sqrt(MSE)
print("Test RMSE :" ,RMSE)

r2 = r2_score(y_test, y_pred_lasso)
print("Test R2 :" ,r2)
print("Test Adjusted R2 : ",1-(1-r2_score(y_test, y_pred_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))

"""#**Applying Ridge Regression to our DataSet**

"""

from sklearn.linear_model import Ridge
### Cross validation
ridge = Ridge()
parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}
ridge_regressor = GridSearchCV(ridge, parameters, scoring='r2', cv=5)
ridge_regressor.fit(X_train, y_train)

ridge_regressor.score(X_train, y_train)

print("The best fit alpha value is found out to be :" ,ridge_regressor.best_params_)
print("\nUsing ",ridge_regressor.best_params_, " the negative mean squared error is: ", lasso_regressor.best_score_)

y_pred_ridge_test = ridge_regressor.predict(X_test)
y_pred_ridge_train=ridge_regressor.predict(X_train)

plt.figure(figsize=(8,5))
plt.title('Train Data')
plt.plot(y_pred_ridge_train)
plt.plot(np.array(y_train))
plt.legend(["Predicted","Actual"])
plt.show()

MSE  = mean_squared_error(y_train, y_pred_ridge_train)
print("Train MSE :" , MSE)

RMSE = np.sqrt(MSE)
print("Train RMSE :" ,RMSE)

r2 = r2_score(y_train, y_pred_ridge_train)
print("Train R2 :" ,r2)
print("Train Adjusted R2 : ",1-(1-r2_score(y_train, y_pred_ridge_train))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)))

plt.figure(figsize=(8,5))
plt.title("Test Data")
plt.plot(y_pred_ridge_test)
plt.plot(np.array(y_test))
plt.legend(["Predicted","Actual"])
plt.show()

MSE  = mean_squared_error(y_test, y_pred_ridge_test)
print("Test MSE :" , MSE)

RMSE = np.sqrt(MSE)
print("Test RMSE :" ,RMSE)

r2 = r2_score(y_test, y_pred_ridge_test)
print("Test R2 :" ,r2)
print("Test Adjusted R2 : ",1-(1-r2_score(y_test, y_pred_ridge_test))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))

sns.distplot(y_test - y_pred_ridge_test ).set_title("error distribution between actual and predicted values")
plt.show()

"""#**Applying XGBRegressor algorithem to our DataSet**"""

# Number of trees
n_estimators = [50,100,120]

# Maximum depth of trees
max_depth = [5,7,9]
min_samples_split = [40,50]
#learning_rate=[0.1,0.3,0.5]

# HYperparameter Grid
param_xgb = {'n_estimators' : n_estimators,
              'max_depth' : max_depth,
             'min_samples_split':min_samples_split
              }

# Create an instance of the  XGBRegressor
import xgboost as xgb
xgb_model = xgb.XGBRegressor()

# Grid search
xgb_grid = GridSearchCV(estimator=xgb_model,param_grid = param_xgb,cv = 3, verbose=2,scoring="r2")

xgb_grid.fit(X_train,y_train)

xgb_grid.best_score_

xgb_grid.best_params_

xgb_optimal_model =xgb_grid.best_estimator_

y_pred_xgb_test=xgb_optimal_model.predict(X_test)
y_pred_xgb_train=xgb_optimal_model.predict(X_train)

plt.figure(figsize=(8,5))
plt.title('Train Data')
plt.plot(y_pred_xgb_train)
plt.plot(np.array(y_train))
plt.legend(["Predicted","Actual"])
plt.show()

MSE  = mean_squared_error(y_train, y_pred_xgb_train)
print("Train MSE :" , MSE)

RMSE = np.sqrt(MSE)
print("Train RMSE :" ,RMSE)

r2 = r2_score(y_train, y_pred_xgb_train)
print("Train R2 :" ,r2)
print("Train Adjusted R2 : ",1-(1-r2_score((y_train), (y_pred_xgb_train)))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)))

plt.figure(figsize=(8,5))
plt.title('Test Data')
plt.plot(y_pred_xgb_test)
plt.plot(np.array(y_test))
plt.legend(["Predicted","Actual"])
plt.show()

MSE  = mean_squared_error(y_test, y_pred_xgb_test)
print("Test MSE :" , MSE)

RMSE = np.sqrt(MSE)
print("Test RMSE :" ,RMSE)

r2 = r2_score(y_test, y_pred_xgb_test)
print("Test R2 :" ,r2)
print("Test Adjusted R2 : ",1-(1-r2_score((y_test), (y_pred_xgb_test)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))

X_train.columns

xgb_optimal_model.feature_importances_

importances = xgb_optimal_model.feature_importances_

importance_dict = {'Feature' : list(X_train.columns),
                   'Feature Importance' : importances}

importance_df = pd.DataFrame(importance_dict)

importance_df['Feature Importance'] = round(importance_df['Feature Importance'],2)

importance_df

importance_df.sort_values(by=['Feature Importance'],ascending=False,inplace=True)

plt.figure(figsize=(17,9))
plt.title('Features importace')
sns.barplot(x='Feature',y="Feature Importance",data=importance_df[:10])

"""#**Applying GradientBoosting Algorithem to our DataSet**"""

# Number of trees
n_estimators = [100,120]

# Maximum depth of trees
max_depth = [5,8,10]

# Minimum number of samples required to split a node
min_samples_split = [50,80]

# Minimum number of samples required at each leaf node
min_samples_leaf = [40,50]


# HYperparameter Grid
param_gb = {'n_estimators' : n_estimators,
              'max_depth' : max_depth,
              'min_samples_split' : min_samples_split,
              'min_samples_leaf' : min_samples_leaf}

# Create an instance of the  GradientBoostingRegressor
from sklearn.ensemble import GradientBoostingRegressor
gb_model=GradientBoostingRegressor()

# Grid search
gb_grid = GridSearchCV(estimator=gb_model,
                       param_grid = param_gb,
                       cv = 3, verbose=2, scoring='r2')

gb_grid.fit(X_train,y_train)

gb_grid.best_params_

gb_grid.best_estimator_

gb_optimal_model = gb_grid.best_estimator_

y_preds_gb = gb_optimal_model.predict(X_test)
y_pred_gb_train=gb_optimal_model.predict(X_train)

plt.figure(figsize=(8,5))
plt.title('Train Data')
plt.plot(y_pred_gb_train)
plt.plot(np.array(y_train))
plt.legend(["Predicted","Actual"])
plt.show()

MSE  = mean_squared_error(y_train, y_pred_gb_train)
print("Train MSE :" , MSE)

RMSE = np.sqrt(MSE)
print("Train RMSE :" ,RMSE)

r2 = r2_score(y_train, y_pred_gb_train)
print("Train R2 :" ,r2)
print("Train Adjusted R2 : ",1-(1-r2_score((y_train), (y_pred_gb_train)))*((X_train.shape[0]-1)/(X_train.shape[0]-X_train.shape[1]-1)))

plt.figure(figsize=(8,5))
plt.title('Test Data')
plt.plot(y_preds_gb)
plt.plot(np.array(y_test))
plt.legend(["Predicted","Actual"])
plt.show()

MSE  = mean_squared_error(y_test, y_preds_gb)
print("Test MSE :" , MSE)

RMSE = np.sqrt(MSE)
print("Test RMSE :" ,RMSE)

r2 = r2_score(y_test, y_preds_gb)
print("Test R2 :" ,r2)
print("Test Adjusted R2 : ",1-(1-r2_score((y_test), (y_preds_gb)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))

xgb_optimal_model.feature_importances_

importances = gb_optimal_model.feature_importances_

importance_dict = {'Feature' : list(X_train.columns),
                   'Feature Importance' : importances}

importance_df = pd.DataFrame(importance_dict)

importance_df['Feature Importance'] = importance_df['Feature Importance']
importance_df

importance_df.sort_values(by=['Feature Importance'],ascending=False,inplace=True)
importance_df

plt.figure(figsize=(17,9))
plt.title('Top 5 Features')
sns.barplot(x='Feature',y="Feature Importance",data=importance_df[:10])

"""#**Model Summary For Train Data**"""

from prettytable import PrettyTable
train = PrettyTable(['SL NO',"MODEL_NAME", "Train MSE", "Train RMSE",'Train R^2','Train Adjusted R^2'])
train.add_row(['1','Lasso Regression','0.005494826110976565','0.07412709431089665','0.49994898506301866','0.4998301932490177'])
train.add_row(['2','Ridge Regression ','0.005494824127596807','0.07412708093265785','0.4999491655584595','0.49983037378733686'])
train.add_row(['3','DecisionTree Regressor','0.003908733073695245','0.06251986143374956','0.6442897552818683','0.6442052529731706'])
train.add_row(['4','XGBRegressor','0.001996779863964856','0.044685342831457114','0.8182850963041854','0.8182419282225373'])
train.add_row(['5','GradientBoosting','0.002278863599313375','0.04773744441539969','0.7926143552635426','0.7925650888563159'])
print(train)

"""#**Model Summary For Test Data**"""

from prettytable import PrettyTable
test = PrettyTable(['SL NO',"MODEL_NAME", "Test MSE", "Test RMSE",'Test R^2','Test Adjusted R^2'])
test.add_row(['1','Lasso Regression','0.005448974432213879','0.07381716895285187','0.5030970331831028','0.502624502834278'])
test.add_row(['2','Ridge Regression ','0.005449008499105121','0.07381739970430495','0.5030939265545915','0.5026213932515153'])
test.add_row(['3','DecisionTree Regressor','0.004203945325941736','0.06483783868962426','0.6166337480963826',' 0.6162691855945723'])
test.add_row(['4','XGBRegressor','0.0031306995630522444','0.0559526546559879','0.7145051935101532','0.7142337019524301'])
test.add_row(['5','GradientBoosting','0.00311712671404449','0.05583123421566543','0.7157429289820318',' 0.7154726144500327'])
print(test)

"""#**From this table we conclude that GradientBoosting perform we for well for our dataset**"""